max_seq_len: 2048
precision: amp_bf16

global_seed: 42
seed: ${global_seed}

# /network/eldar/llama2_7b_c4/oneshot_sparsegpt_sp{40,50,60,70}_nsamples512_seqlen2048
# /network/eldar/mistral_7b_c4/mistral_oneshot_sparsegpt_sp{40,50,60,70}_nsamples512_seqlen2048
# TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
# ahxt/LiteLlama-460M-1T
# microsoft/phi-2

# /network/alexandre/research/cerebras/llama2_7B_sparse50_retrained
# /network/alexandre/research/cerebras/llama2_450M_base

models:
-
  model_name: mistralai/Mistral-7B-v0.1
  model:
    name: hf_causal_lm
    use_flash_attention_2: true
    pretrained: true
    pretrained_model_name_or_path: mistralai/Mistral-7B-v0.1
  tokenizer:
    name: mistralai/Mistral-7B-v0.1
    kwargs:
      model_max_length: ${max_seq_len}
# -
#   model_name: /network/eldar/cerebras/llama2_450M_base
#   model:
#     name: hf_causal_lm
#     use_flash_attention_2: true
#     pretrained: true
#     pretrained_model_name_or_path: /network/eldar/cerebras/llama2_450M_base
#   tokenizer:
#     name: /network/eldar/cerebras/llama2_450M_base
#     kwargs:
#       model_max_length: ${max_seq_len}

device_eval_batch_size: #TODO

# loggers:
#   wandb: {}

fsdp_config:
  sharding_strategy: FULL_SHARD
  mixed_precision: DEFAULT # TODO: vs PURE?
  activation_checkpointing: false
  activation_checkpointing_reentrant: false
  activation_cpu_offload: false
  limit_all_gathers: true

icl_tasks: 'yamls/icl_tasks.yaml'


