max_seq_len: #TODO
precision: amp_bf16
device_eval_batch_size: #TODO

global_seed: 42
seed: ${global_seed}

ckpt_path: #TODO

models:
-
  model_name: ${ckpt_path}
  model:
    name: hf_causal_lm
    use_flash_attention_2: true
    pretrained: true
    pretrained_model_name_or_path: ${ckpt_path}
    use_auth_token: true
  tokenizer:
    name: ${ckpt_path}
    kwargs:
      model_max_length: ${max_seq_len}

fsdp_config:
  sharding_strategy: FULL_SHARD
  mixed_precision: PURE
  activation_checkpointing: false
  activation_checkpointing_reentrant: false
  activation_cpu_offload: false
  limit_all_gathers: true

icl_tasks: "eval/yamls/openllm.yaml"
